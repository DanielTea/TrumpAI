{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[u'both countries will, perhaps, work together to solve some of the many great and pressing problems and issues of the WORLD!',\n",
       " 817753083707015168]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import tweepy\n",
    "import csv\n",
    "\n",
    "#Twitter API credentials\n",
    "consumer_key = 'CHP3W87YAeQDR4tlhQIr8a0TG'\n",
    "consumer_secret = 'QixUh5YjuJdqi5HaYQ3W3ZS1fBAHq2FbqSZ9HcyJidCSWk12Dq'\n",
    "access_key = '817429388778176513-oiFrHxlqFVn21L06583fyyPe15Slj27'\n",
    "access_secret = 'sTk5yAJYw35k3952JvDlHAmYYPE6nh2olBMAVWH9WPpDX'\n",
    "\n",
    "def get_last_tweet(screen_name):\n",
    "    \n",
    "    #Twitter only allows access to a users most recent 3240 tweets with this method\n",
    "    \n",
    "    #authorize twitter, initialize tweepy\n",
    "    auth = tweepy.OAuthHandler(consumer_key, consumer_secret)\n",
    "    auth.set_access_token(access_key, access_secret)\n",
    "    api = tweepy.API(auth)\n",
    "    \n",
    "    tweet = api.user_timeline(screen_name = screen_name, count = 1)[0]\n",
    "    #print(tweet.text)\n",
    "    #print(tweet.id)\n",
    "\n",
    "    return [tweet.text, tweet.id]\n",
    "\n",
    "def get_all_tweets(screen_name):\n",
    "    #Twitter only allows access to a users most recent 3240 tweets with this method\n",
    "    \n",
    "    #authorize twitter, initialize tweepy\n",
    "    auth = tweepy.OAuthHandler(consumer_key, consumer_secret)\n",
    "    auth.set_access_token(access_key, access_secret)\n",
    "    api = tweepy.API(auth)\n",
    "\n",
    "    #initialize a list to hold all the tweepy Tweets\n",
    "    alltweets = []\n",
    "    \n",
    "    #make initial request for most recent tweets (200 is the maximum allowed count)\n",
    "    new_tweets = api.user_timeline(screen_name = screen_name,count=200)\n",
    "\n",
    "    #save most recent tweets\n",
    "    alltweets.extend(new_tweets)\n",
    "    \n",
    "    #save the id of the oldest tweet less one\n",
    "    oldest = alltweets[-1].id - 1\n",
    "   \n",
    "    #keep grabbing tweets until there are no tweets left to grab\n",
    "    while len(new_tweets) > 0:\n",
    "        print \"getting tweets before %s\" % (oldest)\n",
    "\n",
    "        #all subsiquent requests use the max_id param to prevent duplicates\n",
    "        new_tweets = api.user_timeline(screen_name = screen_name,count=200,max_id=oldest)\n",
    "\n",
    "        #save most recent tweets\n",
    "        alltweets.extend(new_tweets)\n",
    "\n",
    "        #update the id of the oldest tweet less one\n",
    "        oldest = alltweets[-1].id - 1\n",
    "\n",
    "    print \"...%s tweets downloaded so far\" % (len(alltweets))\n",
    "\n",
    "    #transform the tweepy tweets into a 2D array that will populate the csv\t\n",
    "    outtweets = [tweet.text.encode(\"utf-8\") for tweet in alltweets]\n",
    "\n",
    "\n",
    "    thefile = open('./data/trumpText/input.txt', 'w')\n",
    "\n",
    "    for item in outtweets:\n",
    "        thefile.write(\"%s\\n\" % item)\n",
    "    \n",
    "    \n",
    "\n",
    "#get_all_tweets(\"realDonaldTrump\")\n",
    "\n",
    "get_last_tweet(\"realDonaldTrump\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Implementation of RAKE - Rapid Automtic Keyword Exraction algorithm\n",
    "# as described in:\n",
    "# Rose, S., D. Engel, N. Cramer, and W. Cowley (2010). \n",
    "# Automatic keyword extraction from indi-vidual documents. \n",
    "# In M. W. Berry and J. Kogan (Eds.), Text Mining: Applications and Theory.unknown: John Wiley and Sons, Ltd.\n",
    "\n",
    "import re\n",
    "import operator\n",
    "\n",
    "debug = False\n",
    "test = True\n",
    "\n",
    "\n",
    "def is_number(s):\n",
    "    try:\n",
    "        float(s) if '.' in s else int(s)\n",
    "        return True\n",
    "    except ValueError:\n",
    "        return False\n",
    "\n",
    "\n",
    "def load_stop_words(stop_word_file):\n",
    "    \"\"\"\n",
    "    Utility function to load stop words from a file and return as a list of words\n",
    "    @param stop_word_file Path and file name of a file containing stop words.\n",
    "    @return list A list of stop words.\n",
    "    \"\"\"\n",
    "    stop_words = []\n",
    "    for line in open(stop_word_file):\n",
    "        if line.strip()[0:1] != \"#\":\n",
    "            for word in line.split():  # in case more than one per line\n",
    "                stop_words.append(word)\n",
    "    return stop_words\n",
    "\n",
    "\n",
    "def separate_words(text, min_word_return_size):\n",
    "    \"\"\"\n",
    "    Utility function to return a list of all words that are have a length greater than a specified number of characters.\n",
    "    @param text The text that must be split in to words.\n",
    "    @param min_word_return_size The minimum no of characters a word must have to be included.\n",
    "    \"\"\"\n",
    "    splitter = re.compile('[^a-zA-Z0-9_\\\\+\\\\-/]')\n",
    "    words = []\n",
    "    for single_word in splitter.split(text):\n",
    "        current_word = single_word.strip().lower()\n",
    "        #leave numbers in phrase, but don't count as words, since they tend to invalidate scores of their phrases\n",
    "        if len(current_word) > min_word_return_size and current_word != '' and not is_number(current_word):\n",
    "            words.append(current_word)\n",
    "    return words\n",
    "\n",
    "\n",
    "def split_sentences(text):\n",
    "    \"\"\"\n",
    "    Utility function to return a list of sentences.\n",
    "    @param text The text that must be split in to sentences.\n",
    "    \"\"\"\n",
    "    sentence_delimiters = re.compile(u'[.!?,;:\\t\\\\\\\\\"\\\\(\\\\)\\\\\\'\\u2019\\u2013]|\\\\s\\\\-\\\\s')\n",
    "    sentences = sentence_delimiters.split(text)\n",
    "    return sentences\n",
    "\n",
    "\n",
    "def build_stop_word_regex(stop_word_file_path):\n",
    "    stop_word_list = load_stop_words(stop_word_file_path)\n",
    "    stop_word_regex_list = []\n",
    "    for word in stop_word_list:\n",
    "        word_regex = r'\\b' + word + r'(?![\\w-])'  # added look ahead for hyphen\n",
    "        stop_word_regex_list.append(word_regex)\n",
    "    stop_word_pattern = re.compile('|'.join(stop_word_regex_list), re.IGNORECASE)\n",
    "    return stop_word_pattern\n",
    "\n",
    "\n",
    "def generate_candidate_keywords(sentence_list, stopword_pattern):\n",
    "    phrase_list = []\n",
    "    for s in sentence_list:\n",
    "        tmp = re.sub(stopword_pattern, '|', s.strip())\n",
    "        phrases = tmp.split(\"|\")\n",
    "        for phrase in phrases:\n",
    "            phrase = phrase.strip().lower()\n",
    "            if phrase != \"\":\n",
    "                phrase_list.append(phrase)\n",
    "    return phrase_list\n",
    "\n",
    "\n",
    "def calculate_word_scores(phraseList):\n",
    "    word_frequency = {}\n",
    "    word_degree = {}\n",
    "    for phrase in phraseList:\n",
    "        word_list = separate_words(phrase, 0)\n",
    "        word_list_length = len(word_list)\n",
    "        word_list_degree = word_list_length - 1\n",
    "        #if word_list_degree > 3: word_list_degree = 3 #exp.\n",
    "        for word in word_list:\n",
    "            word_frequency.setdefault(word, 0)\n",
    "            word_frequency[word] += 1\n",
    "            word_degree.setdefault(word, 0)\n",
    "            word_degree[word] += word_list_degree  #orig.\n",
    "            #word_degree[word] += 1/(word_list_length*1.0) #exp.\n",
    "    for item in word_frequency:\n",
    "        word_degree[item] = word_degree[item] + word_frequency[item]\n",
    "\n",
    "    # Calculate Word scores = deg(w)/frew(w)\n",
    "    word_score = {}\n",
    "    for item in word_frequency:\n",
    "        word_score.setdefault(item, 0)\n",
    "        word_score[item] = word_degree[item] / (word_frequency[item] * 1.0)  #orig.\n",
    "    #word_score[item] = word_frequency[item]/(word_degree[item] * 1.0) #exp.\n",
    "    return word_score\n",
    "\n",
    "\n",
    "def generate_candidate_keyword_scores(phrase_list, word_score):\n",
    "    keyword_candidates = {}\n",
    "    for phrase in phrase_list:\n",
    "        keyword_candidates.setdefault(phrase, 0)\n",
    "        word_list = separate_words(phrase, 0)\n",
    "        candidate_score = 0\n",
    "        for word in word_list:\n",
    "            candidate_score += word_score[word]\n",
    "        keyword_candidates[phrase] = candidate_score\n",
    "    return keyword_candidates\n",
    "\n",
    "\n",
    "class Rake(object):\n",
    "    def __init__(self, stop_words_path):\n",
    "        self.stop_words_path = stop_words_path\n",
    "        self.__stop_words_pattern = build_stop_word_regex(stop_words_path)\n",
    "\n",
    "    def run(self, text):\n",
    "        sentence_list = split_sentences(text)\n",
    "\n",
    "        phrase_list = generate_candidate_keywords(sentence_list, self.__stop_words_pattern)\n",
    "\n",
    "        word_scores = calculate_word_scores(phrase_list)\n",
    "\n",
    "        keyword_candidates = generate_candidate_keyword_scores(phrase_list, word_scores)\n",
    "\n",
    "        sorted_keywords = sorted(keyword_candidates.iteritems(), key=operator.itemgetter(1), reverse=True)\n",
    "        return sorted_keywords\n",
    "\n",
    "\n",
    "def extractKeywords(text):\n",
    "    #text = \"Compatibility of systems of linear constraints over the set of natural numbers. Criteria of compatibility of a system of linear Diophantine equations, strict inequations, and nonstrict inequations are considered. Upper bounds for components of a minimal set of solutions and algorithms of construction of minimal generating sets of solutions for all types of systems are given. These criteria and the corresponding algorithms for constructing a minimal supporting set of solutions can be used in solving all the considered types of systems and systems of mixed types.\"\n",
    "\n",
    "    # Split text into sentences\n",
    "    sentenceList = split_sentences(text)\n",
    "    #stoppath = \"FoxStoplist.txt\" #Fox stoplist contains \"numbers\", so it will not find \"natural numbers\" like in Table 1.1\n",
    "    stoppath = \"./keywordExtractor/SmartStoplist.txt\"  #SMART stoplist misses some of the lower-scoring keywords in Figure 1.5, which means that the top 1/3 cuts off one of the 4.0 score words in Table 1.1\n",
    "    stopwordpattern = build_stop_word_regex(stoppath)\n",
    "\n",
    "    # generate candidate keywords\n",
    "    phraseList = generate_candidate_keywords(sentenceList, stopwordpattern)\n",
    "\n",
    "    # calculate individual word scores\n",
    "    wordscores = calculate_word_scores(phraseList)\n",
    "\n",
    "    # generate candidate keyword scores\n",
    "    keywordcandidates = generate_candidate_keyword_scores(phraseList, wordscores)\n",
    "    if debug: print keywordcandidates\n",
    "\n",
    "    sortedKeywords = sorted(keywordcandidates.iteritems(), key=operator.itemgetter(1), reverse=True)\n",
    "    if debug: print sortedKeywords\n",
    "\n",
    "    totalKeywords = len(sortedKeywords)\n",
    "    if debug: print totalKeywords\n",
    "    #print sortedKeywords[0:(totalKeywords / 3)]\n",
    "\n",
    "    rake = Rake(\"./keywordExtractor/SmartStoplist.txt\")\n",
    "    keywords = rake.run(text)\n",
    "    \n",
    "    return keywords[0][0]\n",
    "    \n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import subprocess as sp\n",
    "import os\n",
    "import shlex\n",
    "import random\n",
    "        \n",
    "\n",
    "def tweetSomText(text):\n",
    "        \n",
    "    keyword = '\\\"'+ extractKeywords(text).encode('utf-8')+ ' \\\"'\n",
    "        \n",
    "        #print keyword\n",
    "        \n",
    "    randomInt = 140 - len(keyword) \n",
    "\n",
    "        #print randomInt\n",
    "        \n",
    "    output = sp.Popen(shlex.split('bash sampleShell.sh '+ keyword + ' ' +str(randomInt)), stdin=sp.PIPE, stdout=sp.PIPE, close_fds=True).communicate()[0]\n",
    "        \n",
    "    Tweets = open('./Tweets.txt', 'w')\n",
    "    Tweets.write(\"%s\\n\" % output)\n",
    "        #print output\n",
    "    \n",
    "    from twython import Twython\n",
    "    twitter = Twython(consumer_key, consumer_secret, access_key, access_secret)\n",
    "        \n",
    "    twitter.update_status(status=output)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import subprocess as sp\n",
    "import os\n",
    "import shlex\n",
    "import random\n",
    "        \n",
    "\n",
    "def tweetSomething():\n",
    "    \n",
    "    with open('./latestTweet.txt', 'r') as f:\n",
    "        first_line = f.readline()\n",
    "        \n",
    "    trumpsLatestTweet = get_last_tweet(\"realDonaldTrump\")\n",
    "    \n",
    "    #print trumpsLatestTweet\n",
    "    if first_line != str(trumpsLatestTweet[1]):\n",
    "    #if first_line != 0:\n",
    "    \n",
    "        latestTweet = open('./latestTweet.txt', 'w')\n",
    "        latestTweet.write(\"%s\" % trumpsLatestTweet[1])\n",
    "        \n",
    "        keyword = '\\\"'+ extractKeywords(trumpsLatestTweet[0]).encode('utf-8')+ ' \\\"'\n",
    "        \n",
    "        #print keyword\n",
    "        \n",
    "        randomInt = 140 - len(keyword) \n",
    "\n",
    "        #print randomInt\n",
    "        \n",
    "        output = sp.Popen(shlex.split('bash sampleShell.sh '+ keyword + ' ' +str(randomInt)), stdin=sp.PIPE, stdout=sp.PIPE, close_fds=True).communicate()[0]\n",
    "        \n",
    "        Tweets = open('./Tweets.txt', 'w')\n",
    "        Tweets.write(\"%s\\n\" % output)\n",
    "        #print output\n",
    "    \n",
    "        from twython import Twython\n",
    "        twitter = Twython(consumer_key, consumer_secret, access_key, access_secret)\n",
    "        \n",
    "        twitter.update_status(status=output)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "UnicodeEncodeError",
     "evalue": "'ascii' codec can't encode character u'\\u2026' in position 37: ordinal not in range(128)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mUnicodeEncodeError\u001b[0m                        Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-66-d3287aecefe5>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mwhile\u001b[0m \u001b[0mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m     \u001b[0mtweetSomething\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m     \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msleep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-65-c11be03bb9b8>\u001b[0m in \u001b[0;36mtweetSomething\u001b[0;34m()\u001b[0m\n\u001b[1;32m     27\u001b[0m         \u001b[0;31m#print randomInt\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 29\u001b[0;31m         \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mshlex\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'bash sampleShell.sh '\u001b[0m\u001b[0;34m+\u001b[0m \u001b[0mkeyword\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m' '\u001b[0m \u001b[0;34m+\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrandomInt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstdin\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPIPE\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstdout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPIPE\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mclose_fds\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcommunicate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     30\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m         \u001b[0mTweets\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'./Tweets.txt'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'w'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python2.7/shlex.pyc\u001b[0m in \u001b[0;36msplit\u001b[0;34m(s, comments, posix)\u001b[0m\n\u001b[1;32m    273\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    274\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcomments\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mposix\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 275\u001b[0;31m     \u001b[0mlex\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mshlex\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mposix\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mposix\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    276\u001b[0m     \u001b[0mlex\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwhitespace_split\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    277\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mcomments\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python2.7/shlex.pyc\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, instream, infile, posix)\u001b[0m\n\u001b[1;32m     23\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minstream\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minfile\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mposix\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minstream\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbasestring\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 25\u001b[0;31m             \u001b[0minstream\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mStringIO\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minstream\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     26\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0minstream\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minstream\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minstream\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mUnicodeEncodeError\u001b[0m: 'ascii' codec can't encode character u'\\u2026' in position 37: ordinal not in range(128)"
     ]
    }
   ],
   "source": [
    "import time\n",
    "while True:\n",
    "    tweetSomething()\n",
    "    time.sleep(5)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
