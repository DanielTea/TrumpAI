{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[u'Thank you to our amazing Wounded Warriors for their service. It was an honor to be with them tonight in D.C.\\u2026 https://t.co/Qj5cpfaykD',\n",
       " 821930600290521089]"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import tweepy\n",
    "import csv\n",
    "\n",
    "#Twitter API credentials\n",
    "consumer_key = ''\n",
    "consumer_secret = ''\n",
    "access_key = ''\n",
    "access_secret = ''\n",
    "\n",
    "def get_last_tweet(screen_name):\n",
    "    \n",
    "    #Twitter only allows access to a users most recent 3240 tweets with this method\n",
    "    \n",
    "    #authorize twitter, initialize tweepy\n",
    "    auth = tweepy.OAuthHandler(consumer_key, consumer_secret)\n",
    "    auth.set_access_token(access_key, access_secret)\n",
    "    api = tweepy.API(auth)\n",
    "    \n",
    "    tweet = api.user_timeline(screen_name = screen_name, count = 1)[0]\n",
    "    \n",
    "    \n",
    "    #print(tweet.text)\n",
    "    #print(tweet.id)\n",
    "\n",
    "    return [tweet.text, tweet.id]\n",
    "\n",
    "def get_tweetarray(tweetid, screen_name):\n",
    "    \n",
    "    auth = tweepy.OAuthHandler(consumer_key, consumer_secret)\n",
    "    auth.set_access_token(access_key, access_secret)\n",
    "    api = tweepy.API(auth)\n",
    "    \n",
    "    tweetarray = api.user_timeline(screen_name = screen_name, count = 30)\n",
    "    count = 0\n",
    "\n",
    "    for i in range(len(tweetarray)):\n",
    "        if str(tweetarray[i].id) == str(tweetid):\n",
    "            break\n",
    "        else:\n",
    "            count += 1\n",
    "    \n",
    "    return tweetarray[0:count]\n",
    "\n",
    "def get_all_tweets(screen_name):\n",
    "    #Twitter only allows access to a users most recent 3240 tweets with this method\n",
    "    \n",
    "    #authorize twitter, initialize tweepy\n",
    "    auth = tweepy.OAuthHandler(consumer_key, consumer_secret)\n",
    "    auth.set_access_token(access_key, access_secret)\n",
    "    api = tweepy.API(auth)\n",
    "\n",
    "    #initialize a list to hold all the tweepy Tweets\n",
    "    alltweets = []\n",
    "    \n",
    "    #make initial request for most recent tweets (200 is the maximum allowed count)\n",
    "    new_tweets = api.user_timeline(screen_name = screen_name,count=200)\n",
    "\n",
    "    #save most recent tweets\n",
    "    alltweets.extend(new_tweets)\n",
    "    \n",
    "    #save the id of the oldest tweet less one\n",
    "    oldest = alltweets[-1].id - 1\n",
    "   \n",
    "    #keep grabbing tweets until there are no tweets left to grab\n",
    "    while len(new_tweets) > 0:\n",
    "        print \"getting tweets before %s\" % (oldest)\n",
    "\n",
    "        #all subsiquent requests use the max_id param to prevent duplicates\n",
    "        new_tweets = api.user_timeline(screen_name = screen_name,count=200,max_id=oldest)\n",
    "\n",
    "        #save most recent tweets\n",
    "        alltweets.extend(new_tweets)\n",
    "\n",
    "        #update the id of the oldest tweet less one\n",
    "        oldest = alltweets[-1].id - 1\n",
    "\n",
    "    print \"...%s tweets downloaded so far\" % (len(alltweets))\n",
    "\n",
    "    #transform the tweepy tweets into a 2D array that will populate the csv\t\n",
    "    outtweets = [tweet.text.encode(\"utf-8\") for tweet in alltweets]\n",
    "\n",
    "\n",
    "    thefile = open('./data/trumpText/input.txt', 'w')\n",
    "\n",
    "    for item in outtweets:\n",
    "        thefile.write(\"%s\\n\" % item)\n",
    "    \n",
    "    \n",
    "\n",
    "#get_all_tweets(\"realDonaldTrump\")\n",
    "\n",
    "get_last_tweet(\"realDonaldTrump\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Implementation of RAKE - Rapid Automtic Keyword Exraction algorithm\n",
    "# as described in:\n",
    "# Rose, S., D. Engel, N. Cramer, and W. Cowley (2010). \n",
    "# Automatic keyword extraction from indi-vidual documents. \n",
    "# In M. W. Berry and J. Kogan (Eds.), Text Mining: Applications and Theory.unknown: John Wiley and Sons, Ltd.\n",
    "\n",
    "import re\n",
    "import operator\n",
    "\n",
    "debug = False\n",
    "test = True\n",
    "\n",
    "\n",
    "def is_number(s):\n",
    "    try:\n",
    "        float(s) if '.' in s else int(s)\n",
    "        return True\n",
    "    except ValueError:\n",
    "        return False\n",
    "\n",
    "\n",
    "def load_stop_words(stop_word_file):\n",
    "    \"\"\"\n",
    "    Utility function to load stop words from a file and return as a list of words\n",
    "    @param stop_word_file Path and file name of a file containing stop words.\n",
    "    @return list A list of stop words.\n",
    "    \"\"\"\n",
    "    stop_words = []\n",
    "    for line in open(stop_word_file):\n",
    "        if line.strip()[0:1] != \"#\":\n",
    "            for word in line.split():  # in case more than one per line\n",
    "                stop_words.append(word)\n",
    "    return stop_words\n",
    "\n",
    "\n",
    "def separate_words(text, min_word_return_size):\n",
    "    \"\"\"\n",
    "    Utility function to return a list of all words that are have a length greater than a specified number of characters.\n",
    "    @param text The text that must be split in to words.\n",
    "    @param min_word_return_size The minimum no of characters a word must have to be included.\n",
    "    \"\"\"\n",
    "    splitter = re.compile('[^a-zA-Z0-9_\\\\+\\\\-/]')\n",
    "    words = []\n",
    "    for single_word in splitter.split(text):\n",
    "        current_word = single_word.strip().lower()\n",
    "        #leave numbers in phrase, but don't count as words, since they tend to invalidate scores of their phrases\n",
    "        if len(current_word) > min_word_return_size and current_word != '' and not is_number(current_word):\n",
    "            words.append(current_word)\n",
    "    return words\n",
    "\n",
    "\n",
    "def split_sentences(text):\n",
    "    \"\"\"\n",
    "    Utility function to return a list of sentences.\n",
    "    @param text The text that must be split in to sentences.\n",
    "    \"\"\"\n",
    "    sentence_delimiters = re.compile(u'[.!?,;:\\t\\\\\\\\\"\\\\(\\\\)\\\\\\'\\u2019\\u2013]|\\\\s\\\\-\\\\s')\n",
    "    sentences = sentence_delimiters.split(text)\n",
    "    return sentences\n",
    "\n",
    "\n",
    "def build_stop_word_regex(stop_word_file_path):\n",
    "    stop_word_list = load_stop_words(stop_word_file_path)\n",
    "    stop_word_regex_list = []\n",
    "    for word in stop_word_list:\n",
    "        word_regex = r'\\b' + word + r'(?![\\w-])'  # added look ahead for hyphen\n",
    "        stop_word_regex_list.append(word_regex)\n",
    "    stop_word_pattern = re.compile('|'.join(stop_word_regex_list), re.IGNORECASE)\n",
    "    return stop_word_pattern\n",
    "\n",
    "\n",
    "def generate_candidate_keywords(sentence_list, stopword_pattern):\n",
    "    phrase_list = []\n",
    "    for s in sentence_list:\n",
    "        tmp = re.sub(stopword_pattern, '|', s.strip())\n",
    "        phrases = tmp.split(\"|\")\n",
    "        for phrase in phrases:\n",
    "            phrase = phrase.strip().lower()\n",
    "            if phrase != \"\":\n",
    "                phrase_list.append(phrase)\n",
    "    return phrase_list\n",
    "\n",
    "\n",
    "def calculate_word_scores(phraseList):\n",
    "    word_frequency = {}\n",
    "    word_degree = {}\n",
    "    for phrase in phraseList:\n",
    "        word_list = separate_words(phrase, 0)\n",
    "        word_list_length = len(word_list)\n",
    "        word_list_degree = word_list_length - 1\n",
    "        #if word_list_degree > 3: word_list_degree = 3 #exp.\n",
    "        for word in word_list:\n",
    "            word_frequency.setdefault(word, 0)\n",
    "            word_frequency[word] += 1\n",
    "            word_degree.setdefault(word, 0)\n",
    "            word_degree[word] += word_list_degree  #orig.\n",
    "            #word_degree[word] += 1/(word_list_length*1.0) #exp.\n",
    "    for item in word_frequency:\n",
    "        word_degree[item] = word_degree[item] + word_frequency[item]\n",
    "\n",
    "    # Calculate Word scores = deg(w)/frew(w)\n",
    "    word_score = {}\n",
    "    for item in word_frequency:\n",
    "        word_score.setdefault(item, 0)\n",
    "        word_score[item] = word_degree[item] / (word_frequency[item] * 1.0)  #orig.\n",
    "    #word_score[item] = word_frequency[item]/(word_degree[item] * 1.0) #exp.\n",
    "    return word_score\n",
    "\n",
    "\n",
    "def generate_candidate_keyword_scores(phrase_list, word_score):\n",
    "    keyword_candidates = {}\n",
    "    for phrase in phrase_list:\n",
    "        keyword_candidates.setdefault(phrase, 0)\n",
    "        word_list = separate_words(phrase, 0)\n",
    "        candidate_score = 0\n",
    "        for word in word_list:\n",
    "            candidate_score += word_score[word]\n",
    "        keyword_candidates[phrase] = candidate_score\n",
    "    return keyword_candidates\n",
    "\n",
    "\n",
    "class Rake(object):\n",
    "    def __init__(self, stop_words_path):\n",
    "        self.stop_words_path = stop_words_path\n",
    "        self.__stop_words_pattern = build_stop_word_regex(stop_words_path)\n",
    "\n",
    "    def run(self, text):\n",
    "        sentence_list = split_sentences(text)\n",
    "\n",
    "        phrase_list = generate_candidate_keywords(sentence_list, self.__stop_words_pattern)\n",
    "\n",
    "        word_scores = calculate_word_scores(phrase_list)\n",
    "\n",
    "        keyword_candidates = generate_candidate_keyword_scores(phrase_list, word_scores)\n",
    "\n",
    "        sorted_keywords = sorted(keyword_candidates.iteritems(), key=operator.itemgetter(1), reverse=True)\n",
    "        return sorted_keywords\n",
    "\n",
    "\n",
    "def extractKeywords(text):\n",
    "    #text = \"Compatibility of systems of linear constraints over the set of natural numbers. Criteria of compatibility of a system of linear Diophantine equations, strict inequations, and nonstrict inequations are considered. Upper bounds for components of a minimal set of solutions and algorithms of construction of minimal generating sets of solutions for all types of systems are given. These criteria and the corresponding algorithms for constructing a minimal supporting set of solutions can be used in solving all the considered types of systems and systems of mixed types.\"\n",
    "\n",
    "    # Split text into sentences\n",
    "    sentenceList = split_sentences(text)\n",
    "    #stoppath = \"FoxStoplist.txt\" #Fox stoplist contains \"numbers\", so it will not find \"natural numbers\" like in Table 1.1\n",
    "    stoppath = \"./keywordExtractor/SmartStoplist.txt\"  #SMART stoplist misses some of the lower-scoring keywords in Figure 1.5, which means that the top 1/3 cuts off one of the 4.0 score words in Table 1.1\n",
    "    stopwordpattern = build_stop_word_regex(stoppath)\n",
    "\n",
    "    # generate candidate keywords\n",
    "    phraseList = generate_candidate_keywords(sentenceList, stopwordpattern)\n",
    "\n",
    "    # calculate individual word scores\n",
    "    wordscores = calculate_word_scores(phraseList)\n",
    "\n",
    "    # generate candidate keyword scores\n",
    "    keywordcandidates = generate_candidate_keyword_scores(phraseList, wordscores)\n",
    "    if debug: print keywordcandidates\n",
    "\n",
    "    sortedKeywords = sorted(keywordcandidates.iteritems(), key=operator.itemgetter(1), reverse=True)\n",
    "    if debug: print sortedKeywords\n",
    "\n",
    "    totalKeywords = len(sortedKeywords)\n",
    "    if debug: print totalKeywords\n",
    "    #print sortedKeywords[0:(totalKeywords / 3)]\n",
    "\n",
    "    rake = Rake(\"./keywordExtractor/SmartStoplist.txt\")\n",
    "    keywords = rake.run(text)\n",
    "    \n",
    "    return keywords[0][0]\n",
    "    \n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import subprocess as sp\n",
    "import os\n",
    "import shlex\n",
    "import random\n",
    "        \n",
    "\n",
    "def tweetSomText(text):\n",
    "        \n",
    "    keyword = '\\\"'+ extractKeywords(text).encode('utf-8')+ ' \\\"'\n",
    "        \n",
    "        #print keyword\n",
    "        \n",
    "    randomInt = 140 - len(keyword)\n",
    "\n",
    "        #print randomInt\n",
    "        \n",
    "    output = sp.Popen(shlex.split('bash sampleShell.sh '+ keyword + ' ' +str(randomInt) +' '+ str(0.7)), stdin=sp.PIPE, stdout=sp.PIPE, close_fds=True).communicate()[0]\n",
    "    \n",
    "    print len(output)\n",
    "    Tweets = open('./Tweets.txt', 'w')\n",
    "    Tweets.write(\"%s\\n\" % output)\n",
    "        #print outputt\n",
    "    \n",
    "    from twython import Twython\n",
    "    twitter = Twython(consumer_key, consumer_secret, access_key, access_secret)\n",
    "        \n",
    "    twitter.update_status(status=output)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "139\n"
     ]
    }
   ],
   "source": [
    "tweetSomText('The \"Unaffordable\" Care Act will soon be history!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import subprocess as sp\n",
    "import os\n",
    "import shlex\n",
    "import random\n",
    "        \n",
    "\n",
    "def tweetSomething():\n",
    "    \n",
    "    with open('./latestTweet.txt', 'r') as f:\n",
    "        first_line = f.readline()\n",
    "        \n",
    "    trumpsLatestTweet = get_last_tweet(\"realDonaldTrump\")\n",
    "    \n",
    "    #print trumpsLatestTweet\n",
    "    if first_line != str(trumpsLatestTweet[1]):\n",
    "    #if first_line != 0:\n",
    "    \n",
    "        \n",
    "        tweetarray = get_tweetarray(first_line, \"realDonaldTrump\")\n",
    "        \n",
    "        #print tweetarray\n",
    "        \n",
    "        for item in tweetarray:\n",
    "        \n",
    "        \n",
    "            keyword = '\\\"'+ extractKeywords(item.text).encode('utf-8')+ ' \\\"'\n",
    "\n",
    "            #print keyword\n",
    "\n",
    "            randomInt = 140 - len(keyword) \n",
    "            randomtemp = random.randint(0,3)\n",
    "            temp=0.7\n",
    "\n",
    "            if randomtemp == 0:\n",
    "                temp = 0.5\n",
    "            elif randomtemp == 1:\n",
    "                temp = 0.6\n",
    "            elif randomtemp == 2:\n",
    "                temp = 0.7\n",
    "            elif randomtemp ==3:\n",
    "                temp = 0.8\n",
    "\n",
    "            #print randomInt\n",
    "\n",
    "            output = sp.Popen(shlex.split('bash sampleShell.sh '+ keyword + ' ' +str(randomInt) +' '+ str(temp)), stdin=sp.PIPE, stdout=sp.PIPE, close_fds=True).communicate()[0]\n",
    "\n",
    "            Tweets = open('./Tweets.txt', 'w')\n",
    "            Tweets.write(\"%s\\n\" % output)\n",
    "            #print output\n",
    "\n",
    "            from twython import Twython\n",
    "            twitter = Twython(consumer_key, consumer_secret, access_key, access_secret)\n",
    "\n",
    "            twitter.update_status(status=output)\n",
    "        \n",
    "        \n",
    "        latestTweet = open('./latestTweet.txt', 'w')\n",
    "        latestTweet.write(\"%s\" % trumpsLatestTweet[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import time\n",
    "while True:\n",
    "    tweetSomething()\n",
    "    time.sleep(60)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
